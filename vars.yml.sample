---
### define network interface names
# set overlay_iface_name to null if you do not want to set up overlay network.
# then, only provider network will be set up.
svc_iface_name: eth0
mgmt_iface_name: eth1
provider_iface_name: eth2
overlay_iface_name: eth3
storage_iface_name: eth4

### ntp
# Specify time servers for control nodes.
# You can use the default ntp.org servers or time servers in your network.
# If servers are offline and there is no time server in your network,
#   set ntp_servers to empty list.
#   Then, the control nodes will be the ntp servers for other nodes.
# ntp_servers: []
ntp_servers:
  - 0.pool.ntp.org
  - 1.pool.ntp.org
  - 2.pool.ntp.org

### keepalived VIP on management network (mandatory)
keepalived_vip: ""
# keepalived VIP on service network (optional)
# Set this if you do not have a direct access to management network
# so you need to access horizon dashboard through service network.
keepalived_vip_svc: ""

### metallb
# To use metallb LoadBalancer, set this to true
metallb_enabled: false
# set up MetalLB LoadBalancer IP range or cidr notation
# IP range: 192.168.20.95-192.168.20.98 (4 IPs can be assigned.)
# CIDR: 192.168.20.128/26 (192.168.20.128 - 191 can be assigned.)
# Only one IP: 192.168.20.95/32
metallb_ip_range: "192.168.20.95-192.168.20.98"

### HA tuning
# ha levels: moderato, allegro, and vivace
# moderato: default liveness update and failover response
# allegro: faster liveness update and failover response
# vivace: fastest liveness update and failover response
ha_level: "moderato"
k8s_ha_level: "moderato"

### storage
# storage backends: ceph and(or) netapp
# If there are multiple backends, the first one is the default backend.
storage_backends:
  - ceph
  - netapp
  - powerflex
  - hitachi

# ceph: set ceph configuration in group_vars/all/ceph_vars.yml
# netapp: set netapp configuration in group_vars/all/netapp_vars.yml
# powerflex: set powerflex configuration in group_vars/all/powerflex_vars.yml
# hitachi: set hitachi configuration in group_vars/all/hitachi_vars.yml

###################################################
## Do not edit below if you are not an expert!!!  #
###################################################
ha:
  moderato:
    interval: 5
    timeout: 3
    rise: 1
    fall: 2
  allegro:
    interval: 4
    timeout: 3
    rise: 1
    fall: 2
  vivace:
    interval: 3
    timeout: 3
    rise: 1
    fall: 2
# keepalived HA parameters
vrrp_script_interval: "{{ ha[ha_level].interval }}"
vrrp_script_timeout: "{{ ha[ha_level].timeout }}"
vrrp_script_rise: "{{ ha[ha_level].rise }}"
vrrp_script_fall: "{{ ha[ha_level].fall }}"
# haproxy HA parameters
inter: "{{ ha[ha_level].interval }}s"
rise: "{{ ha[ha_level].rise }}"
fall: "{{ ha[ha_level].fall }}"

k8s_ha:
  moderato:
    node_status_update_frequency: "10s"
    node_monitor_period: "5s"
    node_monitor_grace_period: "40s"
    not_ready_toleration_seconds: 300
    unreachable_toleration_seconds: 300
    kubelet_shutdown_grace_period: "300s"
    kubelet_shutdown_grace_period_critical_pods: "120s"
  allegro:
    node_status_update_frequency: "10s"
    node_monitor_period: "5s"
    node_monitor_grace_period: "40s"
    not_ready_toleration_seconds: 60
    unreachable_toleration_seconds: 60
    kubelet_shutdown_grace_period: "300s"
    kubelet_shutdown_grace_period_critical_pods: "120s"
  vivace:
    node_status_update_frequency: "6s"
    node_monitor_period: "3s"
    node_monitor_grace_period: "30s"
    not_ready_toleration_seconds: 30
    unreachable_toleration_seconds: 30
    kubelet_shutdown_grace_period: "300s"
    kubelet_shutdown_grace_period_critical_pods: "120s"

# kubespray HA parameters
kubelet_status_update_frequency: "{{ k8s_ha[k8s_ha_level].node_status_update_frequency }}"
kube_controller_node_monitor_period: "{{ k8s_ha[k8s_ha_level].node_monitor_period }}"
kube_controller_node_monitor_grace_period: "{{ k8s_ha[k8s_ha_level].node_monitor_grace_period }}"
kube_apiserver_pod_eviction_not_ready_timeout_seconds: "{{ k8s_ha[k8s_ha_level].not_ready_toleration_seconds }}"
kube_apiserver_pod_eviction_unreachable_timeout_seconds: "{{ k8s_ha[k8s_ha_level].unreachable_toleration_seconds }}"
kubelet_shutdown_grace_period: "{{ k8s_ha[k8s_ha_level].kubelet_shutdown_grace_period }}"
kubelet_shutdown_grace_period_critical_pods: "{{ k8s_ha[k8s_ha_level].kubelet_shutdown_grace_period_critical_pods }}"

# use haproxy as external loadbalancer for kube-apiserver
apiserver_loadbalancer_domain_name: "lb-apiserver.kubernetes.local"
kube_apiserver_bind_address: "{{ ip }}"
kube_apiserver_port: 6443
loadbalancer_apiserver:
  address: "{{ keepalived_vip }}"
  port: "{{ kube_apiserver_port }}"

# deploy_ssh_key: (boolean) create ssh keypair and copy it to other nodes.
# default: true
deploy_ssh_key: true

### MTU setting
calico_mtu: 1500
openstack_mtu: 1500

### neutron
# is_ovs: set false for linuxbridge(default), set true for openvswitch 
is_ovs: false

### vault passwords
ansible_password: "{{ vault_ssh_password }}"
ansible_become_password: "{{ vault_sudo_password }}"

### offline installation flag - default false
# do not change this variable to true
# if you want offline installation, just add offline_vars.yml in extra-vars
# it will override the value - refer to run.sh script.
offline: false

### preflight checklist
checklist:
  quiet: false
  clock_deviation_threshold: 60
  sds_device_min_size_in_mb: 92160
svc_netmask: "{{ hostvars[inventory_hostname]['ansible_' + svc_iface_name].ipv4.network }}/{{ hostvars[inventory_hostname]['ansible_' + svc_iface_name].ipv4.netmask }}"
svc_cidr: "{{ svc_netmask | ansible.utils.ipaddr('net') }}"

### ntp
mgmt_netmask: "{{ hostvars[inventory_hostname]['ansible_' + mgmt_iface_name].ipv4.network }}/{{ hostvars[inventory_hostname]['ansible_' + mgmt_iface_name].ipv4.netmask }}"
ntp_allowed_cidr: "{{ mgmt_netmask | ansible.utils.ipaddr('net') }}"

### keepalived role variables
keepalived_interface: "{{ mgmt_iface_name }}"
# make keepalived service vip optional.
# keepalived service interface vip for future use (default: no setup).
keepalived_interface_svc: "{{ svc_iface_name }}"

### ceph-ansible
# ceph network cidr - recommend the same cidr for public/cluster networks.
storage_netmask: "{{ hostvars[inventory_hostname]['ansible_' + storage_iface_name].ipv4.network }}/{{ hostvars[inventory_hostname]['ansible_' + storage_iface_name].ipv4.netmask }}"
public_network: "{{ storage_netmask | ansible.utils.ipaddr('net') }}"
cluster_network: "{{ public_network }}"
dashboard_enabled: false
configure_firewall: false
ceph_origin: repository
ceph_repository: community
ceph_repository_type: cdn
ceph_stable_release: quincy
osd_objectstore: bluestore
docker_pull_timeout: "60s"
monitor_interface: "{{ storage_iface_name }}"
radosgw_interface: "{{ storage_iface_name }}"

# set the size, min_size, and single_osd_node
ceph_pool_default_size: "{% if (groups['osds']|length) > 3 %}3{% else %}2{% endif %}"
ceph_pool_default_min_size: "{% if (groups['osds']|length) > 3 %}2{% else %}1{% endif %}"
single_osd_node: "{{ ((groups['osds']|length) == 1)|ternary(true, false) }}"

ceph_conf_overrides:
  global:
    auth_allow_insecure_global_id_reclaim: false
    mon_allow_pool_delete: true
    osd_pool_default_size: "{{ ceph_pool_default_size }}"
    osd_pool_default_min_size: "{{ ceph_pool_default_min_size }}"
    osd_crush_chooseleaf_type: "{{ single_osd_node | ternary(0, 1) }}"

# rgw
radosgw_frontend_type: beast
radosgw_frontend_port: 7480

# openstack/k8s pools
openstack_config: true
kube_pool:
  name: "kube"
  application: "rbd"
  target_size_ratio: 0
openstack_glance_pool:
  name: "images"
  application: "rbd"
  target_size_ratio: 0
openstack_cinder_pool:
  name: "volumes"
  application: "rbd"
  target_size_ratio: 0
openstack_cinder_backup_pool:
  name: "backups"
  application: "rbd"
  target_size_ratio: 0
openstack_nova_vms_pool:
  name: "vms"
  application: "rbd"
  target_size_ratio: 0

openstack_basic_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_nova_vms_pool }}"

openstack_pools: "{% if enable_cinder_backup %}{{ openstack_basic_pools + [openstack_cinder_backup_pool] }}{% else %}{{ openstack_basic_pools }}{% endif %}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool={{ kube_pool.name }}", mgr: "profile rbd pool={{ kube_pool.name }}" }, mode: "0600" }
  - { name: client.glance, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder-backup, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }
  - { name: client.openstack, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_glance_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }

# clients
copy_admin_key: true

### kubespray
# pod.replicas is for pods for HA
# quorum.replicas is for pods for quorum like mariadb, rabbitmq.
pod:
  replicas: "{{ (groups['controller-node']|length<2)|ternary(1, 2) }}"
  quorum_replicas: "{{ (groups['controller-node']|length<3) |ternary(1, 3) }}"

upstream_dns_servers:
  - 8.8.8.8
kube_image_repo: "registry.k8s.io"
gcr_image_repo: "gcr.io"
github_image_repo: "ghcr.io"
docker_image_repo: "docker.io"
quay_image_repo: "quay.io"
download_run_once: false
bin_dir: /usr/bin
kube_version: v1.28.3
kube_proxy_strict_arp: true
kubeconfig_localhost: true
kubectl_localhost: true
auto_renew_certificates: true
helm_enabled: true
cert_manager_enabled: true
cert_manager_affinity:
 nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution:
   - weight: 100
     preference:
       matchExpressions:
       - key: node-role.kubernetes.io/control-plane
         operator: In
         values:
         - ""
preinstall_selinux_state: disabled
sysctl_file_path: "/etc/sysctl.d/10-k8s.conf"

# ipam settings for kubernetes
kube_pods_subnet: 10.200.0.0/13
kube_network_node_prefix: 24
kubelet_max_pods: 250

## kube-apiserver
authorization_modes: ['Node', 'RBAC']
kube_apiserver_request_timeout: 120s
kube_apiserver_service_account_lookup: true

# enable kubernetes audit
kubernetes_audit: true
audit_log_path: "/var/log/kube-apiserver-log.json"
audit_log_maxage: 30
audit_log_maxbackups: 10
audit_log_maxsize: 100

tls_min_version: VersionTLS12
tls_cipher_suites:
  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
  - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305

enable_alwayspullimages: true
kube_apiserver_admission_plugins:
  - EventRateLimit
  - NodeRestriction
  - ServiceAccount
  - PodSecurity
kube_apiserver_enable_admission_plugins: "{{ enable_alwayspullimages|ternary(kube_apiserver_admission_plugins|union(['AlwaysPullImages']), kube_apiserver_admission_plugins) }}"
kube_apiserver_admission_control_config_file: true
kube_apiserver_admission_event_rate_limits:
  limit1:
    type: Server
    qps: 5000
    burst: 20000
kube_encrypt_secret_data: true
kube_encryption_resources: [secrets]
kube_encryption_algorithm: "secretbox"
## kube-controller-manager
kube_controller_manager_bind_address: 127.0.0.1
kube_controller_terminated_pod_gc_threshold: 50
kube_controller_feature_gates: ["RotateKubeletServerCertificate=true"]
## kube-scheduler
kube_scheduler_bind_address: 127.0.0.1
## kubelet
kubelet_config_extra_args:
  serializeImagePulls: false
kubelet_authorization_mode_webhook: true
kubelet_authentication_token_webhook: true
kube_read_only_port: 0
kubelet_protect_kernel_defaults: true
kubelet_event_record_qps: 1
kubelet_rotate_certificates: true
kubelet_streaming_connection_idle_timeout: "1h30m"
kubelet_make_iptables_util_chains: true
kubelet_rotate_server_certificates: true
kubelet_feature_gates: ["RotateKubeletServerCertificate=true", "SeccompDefault=true"]
kubelet_seccomp_default: true
kubelet_systemd_hardening: true
kubelet_csr_approver_repository_url: https://postfinance.github.io/kubelet-csr-approver
kubelet_csr_approver_chart_version: 1.0.5
kubelet_csr_approver_values:
  image:
    repository: "{{ docker_image_repo }}/postfinance/kubelet-csr-approver"
    tag: "v{{ kubelet_csr_approver_chart_version }}"
  bypassDnsResolution: true
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""

# create a default Pod Security Configuration and deny running of insecure pods
# kube_system namespace is exempted by default
kube_pod_security_use_default: true
kube_pod_security_default_enforce: restricted

## calico
calico_version: "v3.26.3"
calico_network_backend: bird
calico_ipip_mode: 'Never'
calico_vxlan_mode: 'Never'
# To use node.spec.podCIDR, set this to true.
calico_ipam_host_local: false
calico_pool_blocksize: "{{ kube_network_node_prefix }}"
# override nodeSelector of calico-kube-controllers
calico_policy_controller_deployment_nodeselector: 'node-role.kubernetes.io/control-plane: ""'

## coredns
coredns_deployment_nodeselector: 'node-role.kubernetes.io/control-plane: ""'

## etcd patch variables
# etcd_nice is a nice value from -20(most favorable) to 19(least favorable).
etcd_nice: -10
# etcd scheduling class to use: realtime or best-effort(default)
etcd_ionice_class: best-effort
# etcd scheduler priority(lower is higher): 0(default) - 7
etcd_ionice_priority: 0

# Registry deployment
registry_version: "2.8.2"
registry_enabled: "{{ offline|ternary(true, false) }}"
registry_namespace: kube-system
registry_storage_class: "{{ storage_backends[0] }}"
registry_disk_size: "30Gi"
registry_service_type: "NodePort"
registry_service_nodeport: "32680"
seed_registry_port: "5000"
containerd_registries_mirrors:
  - prefix: docker.io
    protocol: "https"
    mirrors:
      - host: https://registry-1.docker.io
        capabilities: ["pull", "resolve"]
        skip_verify: false

metallb_config:
  controller:
    nodeselector:
      node-role.kubernetes.io/control-plane: ""
  speaker:
    nodeselector:
      node-role.kubernetes.io/control-plane: ""
  address_pools:
    primary:
      ip_range:
        - "{{ metallb_ip_range }}"
      auto_assign: true
  layer2:
    - primary
  layer3:
    defaults:
      peer_port: 179
      hold_time: 120s
    communities: {}
    metallb_peers: {}

### burrito variables
# burrito.haproxy role variables
ceph_rgw_port: 7480

# burrito.ceph-csi role variables
cephcsi_version: "v3.9.0"
csi_attacher_version: "v4.2.0"
csi_node_driver_registrar_version: "v2.6.3"
csi_provisioner_version: "v3.4.0"
csi_resizer_version: "v1.7.0"
csi_snapshotter_version: "v6.2.1"
registry_ready_timeout: 300

# burrito.netapp role variables
trident_version: "22.10.0"

# burrito.powerflex role variables
pfx_version: "v2.6.0"
pfx_snapshot_controller_version: "v6.2.1"

# burrito.openstack role variables
ceph_public_network: "{{ public_network }}"
ceph_cluster_network: "{{ cluster_network }}"

# storageclass_name is used by ingress, mariadb, rabbitmq, glance, nova, btx
# Default: the first one of storage_backends
storageclass_name: "{{ storage_backends[0] }}"
# PVC access modes
pvc_access_modes_map:
  ceph: 'ReadWriteOnce'
  netapp: 'ReadWriteMany'
  powerflex: 'ReadWriteOnce'
  hitachi: 'ReadWriteOnce'


# ingress
ingress:
  volume_size: "100Gi"

# mariadb
mariadb:
  volume_size: "30Gi"
  admin_password: "{{ vault_mariadb_root_password }}"

# rabbitmq
rabbitmq:
  volume_size: "10Gi"
  password: "{{ vault_rabbitmq_openstack_password }}"

# keystone
os_admin_password: "{{ vault_openstack_admin_password }}"
keystone:
  password: "{{ vault_keystone_password }}"

# glance
glance_store_types:
  ceph: 'rbd'
  netapp: 'file'
  powerflex: 'cinder'
  hitachi: 'cinder'
glance_storage:
  ceph: 'rbd'
  netapp: 'pvc'
  powerflex: 'cinder'
  hitachi: 'cinder'
glance_nginx_deploy_types:
  ceph: 'StatefulSet'
  netapp: 'Deployment'
  powerflex: 'StatefulSet'
  hitachi: 'StatefulSet'
glance_pvc_nginx_client_tmp:
  ceph: false
  netapp: true
  powerflex: false
  hitachi: false
glance:
  storage: "{{ storage_backends|map('extract', glance_storage)|list|first }}"
  enabled_backends: "{{ storage_backends|map('extract', glance_store_types)|unique }}"
  default_backend: "{{ storage_backends|map('extract', glance_store_types)|unique|first }}"
  password: "{{ vault_glance_password }}"
  nginx:
    proxy_body_size: "102400M"
    proxy_read_timeout: "3600"
  volume_size: "500Gi"
  nginx_deploy:
    type: "{{ storage_backends|map('extract', glance_nginx_deploy_types)|first }}"
    class_name: "{{ storageclass_name }}"
    access_modes: "{{ storage_backends|map('extract', pvc_access_modes_map)|first }}"
    size: "100Gi"
  pvc_nginx_client_tmp: "{{ storage_backends|map('extract', glance_pvc_nginx_client_tmp)|list|first }}"

# placement
placement:
  password: "{{ vault_placement_password }}"

# libvirt
ceph_secret_uuid: "{{ vault_ceph_secret_uuid }}"

# neutron
bgp_dragent: false
neutron_ml2_plugin: "{{ is_ovs|ternary('openvswitch', 'linuxbridge') }}"
neutron:
  tunnel: "{{ overlay_iface_name }}"
  tunnel_compute: "{{ overlay_iface_name }}"
  password: "{{ vault_neutron_password }}"
ovs_dvr: "{{ is_ovs|ternary(true, false) }}"
ovs_provider:
  - name: external
    bridge: br-ex
    iface: "{{ provider_iface_name }}"
    vlan_ranges: ""
lb_iface_mappings:
  - "external:{{ provider_iface_name }}"

# nova
nova_pvc_instances:
  ceph: false
  netapp: true
  powerflex: false
  hitachi: false
nova:
  vncserver_proxyclient_interface: "{{ mgmt_iface_name }}"
  hypervisor_host_interface: "{{ mgmt_iface_name }}"
  libvirt_live_migration_interface: "{{ mgmt_iface_name }}"
  password: "{{ vault_nova_password }}"
  volume_size: "500Gi"
  pvc_instances: "{{ storage_backends|map('extract', nova_pvc_instances)|list|first }}"

# nova_compute_sshkey_copy
nova_uid: 42424
nova_gid: 42424
ssh_port: "{{ ansible_port }}"

## cinder
# openstack helm cinder chart manifest
enable_cinder_backup: false
enable_cron_volume_usage_audit: false
cinder_backends_map:
  ceph: 'rbd1'
  netapp: "{% for n in netapp %}{{ n.name }}{% if not loop.last %},{% endif %}{% endfor %}"
  powerflex: 'powerflex'
  hitachi: 'hitachi'
cinder_vol_deploy_type_map:
  ceph: 'StatefulSet'
  netapp: 'Deployment'
  powerflex: 'StatefulSet'
  hitachi: 'StatefulSet'
cinder_pvc_conversion:
  ceph: false
  netapp: true
  powerflex: false
  hitachi: false
cinder:
  enabled_backends: "{{ storage_backends|map('extract', cinder_backends_map)|join(',') }}"
  default_volume_type: "{{ storage_backends|map('extract', cinder_backends_map)|first }}"
  password: "{{ vault_cinder_password }}"
  volume_deploy:
    type: "{{ storage_backends|map('extract', cinder_vol_deploy_type_map)|first }}"
    class_name: "{{ storageclass_name }}"
    access_modes: "{{ storage_backends|map('extract', pvc_access_modes_map)|first }}"
    size: "100Gi"
  pvc_conversion: "{{ storage_backends|map('extract', cinder_pvc_conversion)|list|first }}"

# horizon
horizon:
  password: "{{ vault_horizon_password }}"
  timezone: "Asia/Seoul"

# barbican
install_barbican: false
barbican:
  password: "{{ vault_barbican_password }}"
  kek: "{{ vault_barbican_kek | b64encode }}"

### btx
btx:
  version: "1.2.3"
  pvc:
    size: "100Gi"

### dasel binary - used by k8spatch.
dasel_version: v2.4.1
dasel_download_url: "https://github.com/TomWright/dasel/releases/download/{{ dasel_version }}/dasel_linux_amd64"

### asklepios
asklepios:
  version: "0.2.0"
  verbose: false
  sleep: 10
  kickout: 60
  kickin: 60
  balancer: false
  initialDelaySeconds: 5
  periodSeconds: 10
  failureThreshold: 3
  timeoutSeconds: 10
  terminationGracePeriodSeconds: 10
  tolerationSeconds: 20
...
